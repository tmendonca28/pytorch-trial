{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First I need to import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\"\n",
    "        Sigmoid activation function\n",
    "        \n",
    "        Arguments\n",
    "        -----------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating some data\n",
    "torch.manual_seed(7) # Setting the random seed so things will be predictable\n",
    "# Features are 5 random variables\n",
    "features = torch.randn((1,5))\n",
    "# True weights for our data, random normal variables as well\n",
    "weights = torch.randn_like(features)\n",
    "# and a true bias term\n",
    "bias = torch.randn((1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908]])\n"
     ]
    }
   ],
   "source": [
    "print (features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])\n"
     ]
    }
   ],
   "source": [
    "print (weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3177]])\n"
     ]
    }
   ],
   "source": [
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "## Calculating the output of this network using the weights and bias tensors\n",
    "y = activation(torch.sum(features*weights)+bias)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "# We can also do the same using Matrix Multiplications; more efficient\n",
    "# torch.mm() or torch.matmul()\n",
    "##### Use tensor.shape to see the shape of a tensor: Mui importante for NNs #####\n",
    "# 3 different options for reshaping the tensor\n",
    "# tensor.reshape(a,b) will ret new tensor with same data with size (a,b)\n",
    "# tensor.resize_(a,b) ret same tensor with a different shape; this is in place\n",
    "# tensor.view(a,b) most used and ret new tensor with same data with size (a,b)\n",
    "# e.g if my tensor is 1 x 5 I can use tensor_name.view(5, 1) to have it be 5 x 1\n",
    "y_with_matrix_mul = activation(torch.mm(features, weights.view(5, 1)) + bias)\n",
    "print(y_with_matrix_mul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1468,  0.7861,  0.9468]])\n",
      "3\n",
      "tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "# Generating some data\n",
    "torch.manual_seed(7)       # random seed for predicatbility\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features_nn = torch.randn((1, 3))\n",
    "print(features_nn)\n",
    "print(features_nn.shape[1])\n",
    "# Define the size of each layer in our network\n",
    "n_input = features_nn.shape[1]      # Number of input units, must match number of input features\n",
    "n_hidden = 2                     # Number of hidden units\n",
    "n_output = 1                     # Number of output units\n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "weights_1 = torch.randn(n_input, n_hidden)      # (3,2)\n",
    "weights_2 = torch.randn(n_hidden, n_output)     # (2,1)\n",
    "\n",
    "# and the bias terms for hidden and output layers\n",
    "bias_1 = torch.randn((1, n_hidden))\n",
    "bias_2 = torch.randn((1, n_output))\n",
    "\n",
    "input_to_hidden = activation(torch.mm(features_nn, weights_1) + bias_1)\n",
    "hidden_to_output = activation(torch.mm(input_to_hidden, weights_2) + bias_2)\n",
    "print(hidden_to_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A great feature of PyTorch is converting from numpy array to torch tensors\n",
    "# Create tensor from numpy array     torch.from_numpy()\n",
    "# Convert tensor to numpy array      .numpy()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01944541, 0.46829147, 0.30521552],\n",
       "       [0.07805072, 0.56638783, 0.35966326],\n",
       "       [0.46537988, 0.24076658, 0.99945671],\n",
       "       [0.74553878, 0.45632255, 0.68145928]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(4, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0194, 0.4683, 0.3052],\n",
       "        [0.0781, 0.5664, 0.3597],\n",
       "        [0.4654, 0.2408, 0.9995],\n",
       "        [0.7455, 0.4563, 0.6815]], dtype=torch.float64)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating tensor from a\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01944541, 0.46829147, 0.30521552],\n",
       "       [0.07805072, 0.56638783, 0.35966326],\n",
       "       [0.46537988, 0.24076658, 0.99945671],\n",
       "       [0.74553878, 0.45632255, 0.68145928]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.numpy()\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.03889083, 0.93658293, 0.61043105],\n",
       "       [0.15610144, 1.13277567, 0.71932651],\n",
       "       [0.93075975, 0.48153315, 1.99891342],\n",
       "       [1.49107756, 0.91264511, 1.36291856]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the memory is shared between the numpy array and the tensor; in place\n",
    "b.mul_(2)\n",
    "a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
